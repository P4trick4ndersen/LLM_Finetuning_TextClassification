# LLM_Finetuning_TextClassification

This project explores fine-tuning a pre-trained RoBERTa model to classify technical text using a small dataset. The goal is to evaluate how well transformer-based models perform on domain-specific tasks with limited labeled data. The workflow includes data preprocessing, model fine-tuning, and performance evaluation using standard classification metrics.
